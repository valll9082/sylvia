this uses the data set script we just made




from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer
)
from huggingface_hub import login

def login_to_huggingface():
    login()

def load_and_tokenize_dataset(dataset_path, tokenizer):
    dataset = load_dataset('csv', data_files=dataset_path)
    def tokenize_function(examples):
        return tokenizer(examples['text'], padding='max_length', truncation=True)
    tokenized_datasets = dataset.map(tokenize_function, batched=True)
    return tokenized_datasets

def initialize_model_and_tokenizer(model_name):
    model = AutoModelForCausalLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    return model, tokenizer

def setup_training_arguments():
    training_args = TrainingArguments(
        output_dir='./results',
        num_train_epochs=3,
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir='./logs',
        logging_steps=10,
    )
    return training_args

def create_trainer(model, training_args, tokenized_datasets, tokenizer):
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets['train'],
        eval_dataset=tokenized_datasets['train'],  # Using the same dataset for evaluation as a placeholder
        tokenizer=tokenizer,
    )
    return trainer

def main():
    login_to_huggingface()
    model_name = 'SYLVIA'
    dataset_path = 'sample_dataset.csv'  # Path to the dataset
    model, tokenizer = initialize_model_and_tokenizer(model_name)
    tokenized_datasets = load_and_tokenize_dataset(dataset_path, tokenizer)
    training_args = setup_training_arguments()
    trainer = create_trainer(model, training_args, tokenized_datasets, tokenizer)
    trainer.train()

if __name__ == "__main__":
    main()
